{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7qcN5J2FdRNP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7vjfYl5h5sk"
   },
   "source": [
    "**Create training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Nf2p7z57h9mf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(-10, 10, 0.1)\n",
    "n_sample = len(x)\n",
    "\n",
    "noise = np.random.normal(0, 2, n_sample)\n",
    "Y = 2*x - 7 + noise\n",
    "\n",
    "# Vectorize X\n",
    "ones = np.ones(n_sample)\n",
    "X = np.vstack((ones, x)).astype('float32') # Stack arrays in sequence vertically\n",
    "\n",
    "# Use the below line in Jupyter Notebook to view the plot inside notebook, it doesn't work in Google Colab\n",
    "%matplotlib inline\n",
    "plt.plot(x, Y, \"ro\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpEzTc1YdsVE"
   },
   "source": [
    "**Set value:** theta<br>\n",
    "**Define:** loss function<br>\n",
    "**Note:** Do not define the derivative of the loss function because tensorflow will take the derivative by itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1G81SewddqlW"
   },
   "outputs": [],
   "source": [
    "theta = tf.Variable(np.array([[-15.0], # theta_0\n",
    "                              [12.0]], # theta_1\n",
    "                             dtype=np.float32),\n",
    "                    trainable=True\n",
    ")\n",
    "epsilon = 0.001\n",
    "\n",
    "@tf.function\n",
    "def Loss(theta):\n",
    "    Y_pre = tf.matmul(tf.transpose(theta), X)\n",
    "    return tf.reduce_mean(tf.square(Y_pre - Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoRiGPezeZ2P"
   },
   "source": [
    "**Loop:**<br>\n",
    "<span style=\"margin-left:2em\">Update theta</span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U4mCK67ceYEr",
    "outputId": "d01d8f9c-2892-466c-b17d-ccdb84f8cf3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1   Loss 3431.09594727   Theta_1_0: 5.30332041 -14.82936287\n",
      "Epoch: 2   Loss 438.70132446   Theta_1_0: 3.07148814 -14.66883564\n",
      "Epoch: 3   Loss 104.12260437   Theta_1_0: 2.32777882 -14.51375103\n",
      "Epoch: 4   Loss 64.88042450   Theta_1_0: 2.08005571 -14.36251163\n",
      "Epoch: 5   Loss 58.52868652   Theta_1_0: 1.99764061 -14.21454430\n",
      "Epoch: 6   Loss 55.90956879   Theta_1_0: 1.97031975 -14.06961918\n",
      "Epoch: 7   Loss 53.78087997   Theta_1_0: 1.96135867 -13.92761993\n",
      "Epoch: 8   Loss 51.77943420   Theta_1_0: 1.95851386 -13.78846931\n",
      "Epoch: 9   Loss 49.86201096   Theta_1_0: 1.95770490 -13.65210438\n",
      "Epoch: 10   Loss 48.02104187   Theta_1_0: 1.95757163 -13.51846790\n",
      "Epoch: 11   Loss 46.25302887   Theta_1_0: 1.95766079 -13.38750458\n",
      "Epoch: 12   Loss 44.55503082   Theta_1_0: 1.95782149 -13.25916004\n",
      "Epoch: 13   Loss 42.92426682   Theta_1_0: 1.95800340 -13.13338280\n",
      "Epoch: 14   Loss 41.35808563   Theta_1_0: 1.95818985 -13.01012039\n",
      "Epoch: 15   Loss 39.85391617   Theta_1_0: 1.95837522 -12.88932323\n",
      "Epoch: 16   Loss 38.40930176   Theta_1_0: 1.95855784 -12.77094173\n",
      "Epoch: 17   Loss 37.02189636   Theta_1_0: 1.95873713 -12.65492725\n",
      "Epoch: 18   Loss 35.68941879   Theta_1_0: 1.95891285 -12.54123306\n",
      "Epoch: 19   Loss 34.40970612   Theta_1_0: 1.95908511 -12.42981243\n",
      "Epoch: 20   Loss 33.18066788   Theta_1_0: 1.95925391 -12.32062054\n",
      "Epoch: 21   Loss 32.00029755   Theta_1_0: 1.95941937 -12.21361256\n",
      "Epoch: 22   Loss 30.86667061   Theta_1_0: 1.95958149 -12.10874462\n",
      "Epoch: 23   Loss 29.77792931   Theta_1_0: 1.95974040 -12.00597382\n",
      "Epoch: 24   Loss 28.73229980   Theta_1_0: 1.95989621 -11.90525818\n",
      "Epoch: 25   Loss 27.72807312   Theta_1_0: 1.96004879 -11.80655670\n",
      "Epoch: 26   Loss 26.76361084   Theta_1_0: 1.96019840 -11.70982838\n",
      "Epoch: 27   Loss 25.83733177   Theta_1_0: 1.96034491 -11.61503506\n",
      "Epoch: 28   Loss 24.94773674   Theta_1_0: 1.96048856 -11.52213764\n",
      "Epoch: 29   Loss 24.09336853   Theta_1_0: 1.96062934 -11.43109798\n",
      "Epoch: 30   Loss 23.27282715   Theta_1_0: 1.96076727 -11.34187889\n",
      "Epoch: 31   Loss 22.48477936   Theta_1_0: 1.96090257 -11.25444412\n",
      "Epoch: 32   Loss 21.72793388   Theta_1_0: 1.96103501 -11.16875744\n",
      "Epoch: 33   Loss 21.00105476   Theta_1_0: 1.96116483 -11.08478451\n",
      "Epoch: 34   Loss 20.30296135   Theta_1_0: 1.96129215 -11.00249100\n",
      "Epoch: 35   Loss 19.63250923   Theta_1_0: 1.96141684 -10.92184353\n",
      "Epoch: 36   Loss 18.98860550   Theta_1_0: 1.96153903 -10.84280872\n",
      "Epoch: 37   Loss 18.37020111   Theta_1_0: 1.96165884 -10.76535416\n",
      "Epoch: 38   Loss 17.77627754   Theta_1_0: 1.96177614 -10.68944836\n",
      "Epoch: 39   Loss 17.20586967   Theta_1_0: 1.96189129 -10.61506081\n",
      "Epoch: 40   Loss 16.65805244   Theta_1_0: 1.96200395 -10.54216099\n",
      "Epoch: 41   Loss 16.13192558   Theta_1_0: 1.96211445 -10.47071934\n",
      "Epoch: 42   Loss 15.62663555   Theta_1_0: 1.96222270 -10.40070629\n",
      "Epoch: 43   Loss 15.14135265   Theta_1_0: 1.96232879 -10.33209324\n",
      "Epoch: 44   Loss 14.67528343   Theta_1_0: 1.96243274 -10.26485252\n",
      "Epoch: 45   Loss 14.22766972   Theta_1_0: 1.96253467 -10.19895649\n",
      "Epoch: 46   Loss 13.79778099   Theta_1_0: 1.96263456 -10.13437843\n",
      "Epoch: 47   Loss 13.38491440   Theta_1_0: 1.96273232 -10.07109165\n",
      "Epoch: 48   Loss 12.98839760   Theta_1_0: 1.96282828 -10.00907040\n",
      "Epoch: 49   Loss 12.60757828   Theta_1_0: 1.96292222 -9.94828987\n",
      "Epoch: 50   Loss 12.24184227   Theta_1_0: 1.96301436 -9.88872433\n",
      "Epoch: 51   Loss 11.89058495   Theta_1_0: 1.96310461 -9.83034992\n",
      "Epoch: 52   Loss 11.55323505   Theta_1_0: 1.96319306 -9.77314281\n",
      "Epoch: 53   Loss 11.22924328   Theta_1_0: 1.96327972 -9.71708012\n",
      "Epoch: 54   Loss 10.91808319   Theta_1_0: 1.96336472 -9.66213894\n",
      "Epoch: 55   Loss 10.61924458   Theta_1_0: 1.96344793 -9.60829639\n",
      "Epoch: 56   Loss 10.33223915   Theta_1_0: 1.96352959 -9.55553055\n",
      "Epoch: 57   Loss 10.05659771   Theta_1_0: 1.96360958 -9.50381947\n",
      "Epoch: 58   Loss 9.79186916   Theta_1_0: 1.96368790 -9.45314312\n",
      "Epoch: 59   Loss 9.53762436   Theta_1_0: 1.96376467 -9.40347958\n",
      "Epoch: 60   Loss 9.29344463   Theta_1_0: 1.96384001 -9.35480976\n",
      "Epoch: 61   Loss 9.05893707   Theta_1_0: 1.96391380 -9.30711269\n",
      "Epoch: 62   Loss 8.83371162   Theta_1_0: 1.96398604 -9.26037025\n",
      "Epoch: 63   Loss 8.61740875   Theta_1_0: 1.96405685 -9.21456242\n",
      "Epoch: 64   Loss 8.40966892   Theta_1_0: 1.96412623 -9.16967010\n",
      "Epoch: 65   Loss 8.21015263   Theta_1_0: 1.96419430 -9.12567616\n",
      "Epoch: 66   Loss 8.01854038   Theta_1_0: 1.96426094 -9.08256149\n",
      "Epoch: 67   Loss 7.83451176   Theta_1_0: 1.96432626 -9.04030895\n",
      "Epoch: 68   Loss 7.65776968   Theta_1_0: 1.96439028 -8.99890137\n",
      "Epoch: 69   Loss 7.48802757   Theta_1_0: 1.96445310 -8.95832253\n",
      "Epoch: 70   Loss 7.32500744   Theta_1_0: 1.96451461 -8.91855526\n",
      "Epoch: 71   Loss 7.16844368   Theta_1_0: 1.96457481 -8.87958336\n",
      "Epoch: 72   Loss 7.01807928   Theta_1_0: 1.96463382 -8.84139061\n",
      "Epoch: 73   Loss 6.87366819   Theta_1_0: 1.96469176 -8.80396175\n",
      "Epoch: 74   Loss 6.73497581   Theta_1_0: 1.96474850 -8.76728153\n",
      "Epoch: 75   Loss 6.60177565   Theta_1_0: 1.96480405 -8.73133469\n",
      "Epoch: 76   Loss 6.47384834   Theta_1_0: 1.96485853 -8.69610691\n",
      "Epoch: 77   Loss 6.35098886   Theta_1_0: 1.96491194 -8.66158295\n",
      "Epoch: 78   Loss 6.23298931   Theta_1_0: 1.96496427 -8.62774944\n",
      "Epoch: 79   Loss 6.11966419   Theta_1_0: 1.96501553 -8.59459305\n",
      "Epoch: 80   Loss 6.01082754   Theta_1_0: 1.96506572 -8.56209946\n",
      "Epoch: 81   Loss 5.90630007   Theta_1_0: 1.96511495 -8.53025532\n",
      "Epoch: 82   Loss 5.80591011   Theta_1_0: 1.96516323 -8.49904823\n",
      "Epoch: 83   Loss 5.70949602   Theta_1_0: 1.96521056 -8.46846581\n",
      "Epoch: 84   Loss 5.61690140   Theta_1_0: 1.96525681 -8.43849468\n",
      "Epoch: 85   Loss 5.52797222   Theta_1_0: 1.96530223 -8.40912247\n",
      "Epoch: 86   Loss 5.44256306   Theta_1_0: 1.96534681 -8.38033772\n",
      "Epoch: 87   Loss 5.36053610   Theta_1_0: 1.96539044 -8.35212898\n",
      "Epoch: 88   Loss 5.28175735   Theta_1_0: 1.96543312 -8.32448387\n",
      "Epoch: 89   Loss 5.20609808   Theta_1_0: 1.96547508 -8.29739189\n",
      "Epoch: 90   Loss 5.13343430   Theta_1_0: 1.96551609 -8.27084160\n",
      "Epoch: 91   Loss 5.06364727   Theta_1_0: 1.96555638 -8.24482250\n",
      "Epoch: 92   Loss 4.99662590   Theta_1_0: 1.96559572 -8.21932411\n",
      "Epoch: 93   Loss 4.93225813   Theta_1_0: 1.96563435 -8.19433498\n",
      "Epoch: 94   Loss 4.87043715   Theta_1_0: 1.96567225 -8.16984558\n",
      "Epoch: 95   Loss 4.81106424   Theta_1_0: 1.96570933 -8.14584637\n",
      "Epoch: 96   Loss 4.75404406   Theta_1_0: 1.96574569 -8.12232685\n",
      "Epoch: 97   Loss 4.69928026   Theta_1_0: 1.96578133 -8.09927750\n",
      "Epoch: 98   Loss 4.64668512   Theta_1_0: 1.96581626 -8.07668972\n",
      "Epoch: 99   Loss 4.59617376   Theta_1_0: 1.96585059 -8.05455303\n",
      "Epoch: 100   Loss 4.54766178   Theta_1_0: 1.96588409 -8.03285980\n",
      "Epoch: 101   Loss 4.50107193   Theta_1_0: 1.96591699 -8.01159954\n",
      "Epoch: 102   Loss 4.45632410   Theta_1_0: 1.96594918 -7.99076509\n",
      "Epoch: 103   Loss 4.41335011   Theta_1_0: 1.96598077 -7.97034693\n",
      "Epoch: 104   Loss 4.37207747   Theta_1_0: 1.96601176 -7.95033741\n",
      "Epoch: 105   Loss 4.33243942   Theta_1_0: 1.96604204 -7.93072796\n",
      "Epoch: 106   Loss 4.29437113   Theta_1_0: 1.96607172 -7.91151047\n",
      "Epoch: 107   Loss 4.25780964   Theta_1_0: 1.96610081 -7.89267731\n",
      "Epoch: 108   Loss 4.22269535   Theta_1_0: 1.96612942 -7.87422085\n",
      "Epoch: 109   Loss 4.18897200   Theta_1_0: 1.96615744 -7.85613346\n",
      "Epoch: 110   Loss 4.15658379   Theta_1_0: 1.96618485 -7.83840799\n",
      "Epoch: 111   Loss 4.12547874   Theta_1_0: 1.96621168 -7.82103682\n",
      "Epoch: 112   Loss 4.09560490   Theta_1_0: 1.96623802 -7.80401325\n",
      "Epoch: 113   Loss 4.06691408   Theta_1_0: 1.96626377 -7.78733015\n",
      "Epoch: 114   Loss 4.03935957   Theta_1_0: 1.96628904 -7.77098036\n",
      "Epoch: 115   Loss 4.01289558   Theta_1_0: 1.96631384 -7.75495768\n",
      "Epoch: 116   Loss 3.98747921   Theta_1_0: 1.96633816 -7.73925543\n",
      "Epoch: 117   Loss 3.96307063   Theta_1_0: 1.96636188 -7.72386742\n",
      "Epoch: 118   Loss 3.93962741   Theta_1_0: 1.96638525 -7.70878696\n",
      "Epoch: 119   Loss 3.91711307   Theta_1_0: 1.96640813 -7.69400835\n",
      "Epoch: 120   Loss 3.89549065   Theta_1_0: 1.96643054 -7.67952490\n",
      "Epoch: 121   Loss 3.87472343   Theta_1_0: 1.96645248 -7.66533136\n",
      "Epoch: 122   Loss 3.85477853   Theta_1_0: 1.96647394 -7.65142155\n",
      "Epoch: 123   Loss 3.83562398   Theta_1_0: 1.96649504 -7.63778973\n",
      "Epoch: 124   Loss 3.81722736   Theta_1_0: 1.96651566 -7.62443066\n",
      "Epoch: 125   Loss 3.79955935   Theta_1_0: 1.96653593 -7.61133862\n",
      "Epoch: 126   Loss 3.78259087   Theta_1_0: 1.96655571 -7.59850883\n",
      "Epoch: 127   Loss 3.76629448   Theta_1_0: 1.96657515 -7.58593559\n",
      "Epoch: 128   Loss 3.75064397   Theta_1_0: 1.96659422 -7.57361364\n",
      "Epoch: 129   Loss 3.73561287   Theta_1_0: 1.96661294 -7.56153822\n",
      "Epoch: 130   Loss 3.72117686   Theta_1_0: 1.96663117 -7.54970407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 131   Loss 3.70731163   Theta_1_0: 1.96664917 -7.53810644\n",
      "Epoch: 132   Loss 3.69399595   Theta_1_0: 1.96666670 -7.52674103\n",
      "Epoch: 133   Loss 3.68120813   Theta_1_0: 1.96668398 -7.51560307\n",
      "Epoch: 134   Loss 3.66892648   Theta_1_0: 1.96670079 -7.50468779\n",
      "Epoch: 135   Loss 3.65713096   Theta_1_0: 1.96671736 -7.49399042\n",
      "Epoch: 136   Loss 3.64580226   Theta_1_0: 1.96673357 -7.48350716\n",
      "Epoch: 137   Loss 3.63492227   Theta_1_0: 1.96674943 -7.47323370\n",
      "Epoch: 138   Loss 3.62447333   Theta_1_0: 1.96676505 -7.46316576\n",
      "Epoch: 139   Loss 3.61443782   Theta_1_0: 1.96678030 -7.45329905\n",
      "Epoch: 140   Loss 3.60479975   Theta_1_0: 1.96679521 -7.44362974\n",
      "Epoch: 141   Loss 3.59554386   Theta_1_0: 1.96680987 -7.43415356\n",
      "Epoch: 142   Loss 3.58665395   Theta_1_0: 1.96682429 -7.42486715\n",
      "Epoch: 143   Loss 3.57811618   Theta_1_0: 1.96683836 -7.41576624\n",
      "Epoch: 144   Loss 3.56991649   Theta_1_0: 1.96685207 -7.40684748\n",
      "Epoch: 145   Loss 3.56204104   Theta_1_0: 1.96686566 -7.39810705\n",
      "Epoch: 146   Loss 3.55447817   Theta_1_0: 1.96687889 -7.38954115\n",
      "Epoch: 147   Loss 3.54721403   Theta_1_0: 1.96689188 -7.38114691\n",
      "Epoch: 148   Loss 3.54023838   Theta_1_0: 1.96690464 -7.37292051\n",
      "Epoch: 149   Loss 3.53353810   Theta_1_0: 1.96691704 -7.36485863\n",
      "Epoch: 150   Loss 3.52710390   Theta_1_0: 1.96692920 -7.35695791\n",
      "Epoch: 151   Loss 3.52092409   Theta_1_0: 1.96694124 -7.34921503\n",
      "Epoch: 152   Loss 3.51498890   Theta_1_0: 1.96695292 -7.34162712\n",
      "Epoch: 153   Loss 3.50928855   Theta_1_0: 1.96696448 -7.33419085\n",
      "Epoch: 154   Loss 3.50381398   Theta_1_0: 1.96697569 -7.32690334\n",
      "Epoch: 155   Loss 3.49855661   Theta_1_0: 1.96698678 -7.31976175\n",
      "Epoch: 156   Loss 3.49350715   Theta_1_0: 1.96699762 -7.31276274\n",
      "Epoch: 157   Loss 3.48865724   Theta_1_0: 1.96700823 -7.30590391\n",
      "Epoch: 158   Loss 3.48399997   Theta_1_0: 1.96701860 -7.29918194\n",
      "Epoch: 159   Loss 3.47952628   Theta_1_0: 1.96702874 -7.29259443\n",
      "Epoch: 160   Loss 3.47523069   Theta_1_0: 1.96703875 -7.28613901\n",
      "Epoch: 161   Loss 3.47110534   Theta_1_0: 1.96704853 -7.27981234\n",
      "Epoch: 162   Loss 3.46714234   Theta_1_0: 1.96705818 -7.27361250\n",
      "Epoch: 163   Loss 3.46333671   Theta_1_0: 1.96706760 -7.26753664\n",
      "Epoch: 164   Loss 3.45968270   Theta_1_0: 1.96707678 -7.26158190\n",
      "Epoch: 165   Loss 3.45617199   Theta_1_0: 1.96708572 -7.25574636\n",
      "Epoch: 166   Loss 3.45280027   Theta_1_0: 1.96709454 -7.25002766\n",
      "Epoch: 167   Loss 3.44956303   Theta_1_0: 1.96710324 -7.24442339\n",
      "Epoch: 168   Loss 3.44645357   Theta_1_0: 1.96711171 -7.23893118\n",
      "Epoch: 169   Loss 3.44346738   Theta_1_0: 1.96712005 -7.23354864\n",
      "Epoch: 170   Loss 3.44059896   Theta_1_0: 1.96712828 -7.22827387\n",
      "Epoch: 171   Loss 3.43784475   Theta_1_0: 1.96713626 -7.22310448\n",
      "Epoch: 172   Loss 3.43519902   Theta_1_0: 1.96714413 -7.21803856\n",
      "Epoch: 173   Loss 3.43265843   Theta_1_0: 1.96715176 -7.21307373\n",
      "Epoch: 174   Loss 3.43021798   Theta_1_0: 1.96715927 -7.20820856\n",
      "Epoch: 175   Loss 3.42787480   Theta_1_0: 1.96716666 -7.20344067\n",
      "Epoch: 176   Loss 3.42562437   Theta_1_0: 1.96717381 -7.19876814\n",
      "Epoch: 177   Loss 3.42346263   Theta_1_0: 1.96718097 -7.19418907\n",
      "Epoch: 178   Loss 3.42138672   Theta_1_0: 1.96718788 -7.18970156\n",
      "Epoch: 179   Loss 3.41939306   Theta_1_0: 1.96719468 -7.18530369\n",
      "Epoch: 180   Loss 3.41747808   Theta_1_0: 1.96720135 -7.18099356\n",
      "Epoch: 181   Loss 3.41563892   Theta_1_0: 1.96720791 -7.17676973\n",
      "Epoch: 182   Loss 3.41387296   Theta_1_0: 1.96721435 -7.17263031\n",
      "Epoch: 183   Loss 3.41217661   Theta_1_0: 1.96722054 -7.16857386\n",
      "Epoch: 184   Loss 3.41054749   Theta_1_0: 1.96722674 -7.16459846\n",
      "Epoch: 185   Loss 3.40898275   Theta_1_0: 1.96723270 -7.16070271\n",
      "Epoch: 186   Loss 3.40748024   Theta_1_0: 1.96723866 -7.15688467\n",
      "Epoch: 187   Loss 3.40603709   Theta_1_0: 1.96724439 -7.15314293\n",
      "Epoch: 188   Loss 3.40465117   Theta_1_0: 1.96725011 -7.14947605\n",
      "Epoch: 189   Loss 3.40332031   Theta_1_0: 1.96725571 -7.14588261\n",
      "Epoch: 190   Loss 3.40204167   Theta_1_0: 1.96726108 -7.14236116\n",
      "Epoch: 191   Loss 3.40081429   Theta_1_0: 1.96726644 -7.13890982\n",
      "Epoch: 192   Loss 3.39963460   Theta_1_0: 1.96727169 -7.13552761\n",
      "Epoch: 193   Loss 3.39850211   Theta_1_0: 1.96727681 -7.13221312\n",
      "Epoch: 194   Loss 3.39741445   Theta_1_0: 1.96728182 -7.12896490\n",
      "Epoch: 195   Loss 3.39637017   Theta_1_0: 1.96728671 -7.12578154\n",
      "Epoch: 196   Loss 3.39536691   Theta_1_0: 1.96729159 -7.12266207\n",
      "Epoch: 197   Loss 3.39440346   Theta_1_0: 1.96729624 -7.11960506\n",
      "Epoch: 198   Loss 3.39347839   Theta_1_0: 1.96730089 -7.11660910\n",
      "Epoch: 199   Loss 3.39258933   Theta_1_0: 1.96730542 -7.11367273\n",
      "Epoch: 200   Loss 3.39173579   Theta_1_0: 1.96730983 -7.11079550\n",
      "Epoch: 201   Loss 3.39091682   Theta_1_0: 1.96731424 -7.10797548\n",
      "Epoch: 202   Loss 3.39012909   Theta_1_0: 1.96731853 -7.10521173\n",
      "Epoch: 203   Loss 3.38937283   Theta_1_0: 1.96732271 -7.10250330\n",
      "Epoch: 204   Loss 3.38864684   Theta_1_0: 1.96732688 -7.09984922\n",
      "Epoch: 205   Loss 3.38794923   Theta_1_0: 1.96733081 -7.09724808\n",
      "Epoch: 206   Loss 3.38727999   Theta_1_0: 1.96733475 -7.09469891\n",
      "Epoch: 207   Loss 3.38663602   Theta_1_0: 1.96733868 -7.09220076\n",
      "Epoch: 208   Loss 3.38601828   Theta_1_0: 1.96734250 -7.08975267\n",
      "Epoch: 209   Loss 3.38542533   Theta_1_0: 1.96734619 -7.08735371\n",
      "Epoch: 210   Loss 3.38485527   Theta_1_0: 1.96734977 -7.08500242\n",
      "Epoch: 211   Loss 3.38430786   Theta_1_0: 1.96735334 -7.08269835\n",
      "Epoch: 212   Loss 3.38378239   Theta_1_0: 1.96735680 -7.08044052\n",
      "Epoch: 213   Loss 3.38327765   Theta_1_0: 1.96736026 -7.07822752\n",
      "Epoch: 214   Loss 3.38279295   Theta_1_0: 1.96736360 -7.07605886\n",
      "Epoch: 215   Loss 3.38232732   Theta_1_0: 1.96736693 -7.07393360\n",
      "Epoch: 216   Loss 3.38187981   Theta_1_0: 1.96737015 -7.07185078\n",
      "Epoch: 217   Loss 3.38145089   Theta_1_0: 1.96737325 -7.06980991\n",
      "Epoch: 218   Loss 3.38103843   Theta_1_0: 1.96737635 -7.06780958\n",
      "Epoch: 219   Loss 3.38064218   Theta_1_0: 1.96737933 -7.06584930\n",
      "Epoch: 220   Loss 3.38026190   Theta_1_0: 1.96738231 -7.06392813\n",
      "Epoch: 221   Loss 3.37989616   Theta_1_0: 1.96738529 -7.06204557\n",
      "Epoch: 222   Loss 3.37954521   Theta_1_0: 1.96738815 -7.06020069\n",
      "Epoch: 223   Loss 3.37920856   Theta_1_0: 1.96739089 -7.05839252\n",
      "Epoch: 224   Loss 3.37888479   Theta_1_0: 1.96739364 -7.05662060\n",
      "Epoch: 225   Loss 3.37857389   Theta_1_0: 1.96739638 -7.05488396\n",
      "Epoch: 226   Loss 3.37827539   Theta_1_0: 1.96739900 -7.05318213\n",
      "Epoch: 227   Loss 3.37798882   Theta_1_0: 1.96740162 -7.05151415\n",
      "Epoch: 228   Loss 3.37771320   Theta_1_0: 1.96740413 -7.04987955\n",
      "Epoch: 229   Loss 3.37744880   Theta_1_0: 1.96740663 -7.04827785\n",
      "Epoch: 230   Loss 3.37719488   Theta_1_0: 1.96740901 -7.04670811\n",
      "Epoch: 231   Loss 3.37695122   Theta_1_0: 1.96741140 -7.04516983\n",
      "Epoch: 232   Loss 3.37671685   Theta_1_0: 1.96741366 -7.04366255\n",
      "Epoch: 233   Loss 3.37649179   Theta_1_0: 1.96741593 -7.04218531\n",
      "Epoch: 234   Loss 3.37627602   Theta_1_0: 1.96741819 -7.04073763\n",
      "Epoch: 235   Loss 3.37606812   Theta_1_0: 1.96742046 -7.03931856\n",
      "Epoch: 236   Loss 3.37586880   Theta_1_0: 1.96742260 -7.03792810\n",
      "Epoch: 237   Loss 3.37567759   Theta_1_0: 1.96742463 -7.03656530\n",
      "Epoch: 238   Loss 3.37549305   Theta_1_0: 1.96742678 -7.03522968\n",
      "Epoch: 239   Loss 3.37531710   Theta_1_0: 1.96742880 -7.03392076\n",
      "Epoch: 240   Loss 3.37514710   Theta_1_0: 1.96743083 -7.03263807\n",
      "Epoch: 241   Loss 3.37498403   Theta_1_0: 1.96743274 -7.03138113\n",
      "Epoch: 242   Loss 3.37482858   Theta_1_0: 1.96743464 -7.03014946\n",
      "Epoch: 243   Loss 3.37467837   Theta_1_0: 1.96743643 -7.02894211\n",
      "Epoch: 244   Loss 3.37453365   Theta_1_0: 1.96743834 -7.02775908\n",
      "Epoch: 245   Loss 3.37439513   Theta_1_0: 1.96744013 -7.02659988\n",
      "Epoch: 246   Loss 3.37426209   Theta_1_0: 1.96744180 -7.02546358\n",
      "Epoch: 247   Loss 3.37413430   Theta_1_0: 1.96744359 -7.02435017\n",
      "Epoch: 248   Loss 3.37401175   Theta_1_0: 1.96744525 -7.02325916\n",
      "Epoch: 249   Loss 3.37389398   Theta_1_0: 1.96744692 -7.02218962\n",
      "Epoch: 250   Loss 3.37378049   Theta_1_0: 1.96744859 -7.02114153\n",
      "Epoch: 251   Loss 3.37367177   Theta_1_0: 1.96745014 -7.02011442\n",
      "Epoch: 252   Loss 3.37356710   Theta_1_0: 1.96745169 -7.01910782\n",
      "Epoch: 253   Loss 3.37346721   Theta_1_0: 1.96745324 -7.01812172\n",
      "Epoch: 254   Loss 3.37337041   Theta_1_0: 1.96745467 -7.01715517\n",
      "Epoch: 255   Loss 3.37327814   Theta_1_0: 1.96745610 -7.01620770\n",
      "Epoch: 256   Loss 3.37318897   Theta_1_0: 1.96745753 -7.01527929\n",
      "Epoch: 257   Loss 3.37310362   Theta_1_0: 1.96745896 -7.01436949\n",
      "Epoch: 258   Loss 3.37302184   Theta_1_0: 1.96746039 -7.01347780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 259   Loss 3.37294340   Theta_1_0: 1.96746171 -7.01260424\n",
      "Epoch: 260   Loss 3.37286806   Theta_1_0: 1.96746302 -7.01174784\n",
      "Epoch: 261   Loss 3.37279534   Theta_1_0: 1.96746433 -7.01090860\n",
      "Epoch: 262   Loss 3.37272549   Theta_1_0: 1.96746564 -7.01008606\n",
      "Epoch: 263   Loss 3.37265873   Theta_1_0: 1.96746683 -7.00928020\n",
      "Epoch: 264   Loss 3.37259459   Theta_1_0: 1.96746802 -7.00849056\n",
      "Epoch: 265   Loss 3.37253237   Theta_1_0: 1.96746933 -7.00771666\n",
      "Epoch: 266   Loss 3.37247324   Theta_1_0: 1.96747053 -7.00695801\n",
      "Epoch: 267   Loss 3.37241650   Theta_1_0: 1.96747160 -7.00621462\n",
      "Epoch: 268   Loss 3.37236142   Theta_1_0: 1.96747279 -7.00548601\n",
      "Epoch: 269   Loss 3.37230897   Theta_1_0: 1.96747386 -7.00477219\n",
      "Epoch: 270   Loss 3.37225866   Theta_1_0: 1.96747494 -7.00407267\n",
      "Epoch: 271   Loss 3.37221003   Theta_1_0: 1.96747601 -7.00338697\n",
      "Epoch: 272   Loss 3.37216377   Theta_1_0: 1.96747708 -7.00271511\n",
      "Epoch: 273   Loss 3.37211895   Theta_1_0: 1.96747804 -7.00205660\n",
      "Epoch: 274   Loss 3.37207580   Theta_1_0: 1.96747899 -7.00141144\n",
      "Epoch: 275   Loss 3.37203503   Theta_1_0: 1.96747994 -7.00077915\n",
      "Epoch: 276   Loss 3.37199521   Theta_1_0: 1.96748102 -7.00015926\n",
      "Epoch: 277   Loss 3.37195706   Theta_1_0: 1.96748197 -6.99955177\n",
      "Epoch: 278   Loss 3.37192082   Theta_1_0: 1.96748281 -6.99895668\n",
      "Epoch: 279   Loss 3.37188530   Theta_1_0: 1.96748376 -6.99837351\n",
      "Epoch: 280   Loss 3.37185192   Theta_1_0: 1.96748459 -6.99780178\n",
      "Epoch: 281   Loss 3.37181950   Theta_1_0: 1.96748543 -6.99724150\n",
      "Epoch: 282   Loss 3.37178826   Theta_1_0: 1.96748626 -6.99669266\n",
      "Epoch: 283   Loss 3.37175846   Theta_1_0: 1.96748710 -6.99615479\n",
      "Epoch: 284   Loss 3.37172985   Theta_1_0: 1.96748793 -6.99562740\n",
      "Epoch: 285   Loss 3.37170219   Theta_1_0: 1.96748877 -6.99511051\n",
      "Epoch: 286   Loss 3.37167597   Theta_1_0: 1.96748960 -6.99460411\n",
      "Epoch: 287   Loss 3.37165046   Theta_1_0: 1.96749032 -6.99410772\n",
      "Epoch: 288   Loss 3.37162638   Theta_1_0: 1.96749103 -6.99362135\n",
      "Epoch: 289   Loss 3.37160277   Theta_1_0: 1.96749187 -6.99314451\n",
      "Epoch: 290   Loss 3.37158012   Theta_1_0: 1.96749258 -6.99267721\n",
      "Epoch: 291   Loss 3.37155819   Theta_1_0: 1.96749330 -6.99221945\n",
      "Epoch: 292   Loss 3.37153816   Theta_1_0: 1.96749401 -6.99177074\n",
      "Epoch: 293   Loss 3.37151790   Theta_1_0: 1.96749461 -6.99133110\n",
      "Epoch: 294   Loss 3.37149906   Theta_1_0: 1.96749532 -6.99090004\n",
      "Epoch: 295   Loss 3.37148046   Theta_1_0: 1.96749592 -6.99047804\n",
      "Epoch: 296   Loss 3.37146306   Theta_1_0: 1.96749663 -6.99006414\n",
      "Epoch: 297   Loss 3.37144566   Theta_1_0: 1.96749723 -6.98965883\n",
      "Epoch: 298   Loss 3.37142944   Theta_1_0: 1.96749783 -6.98926163\n",
      "Epoch: 299   Loss 3.37141418   Theta_1_0: 1.96749842 -6.98887205\n",
      "Epoch: 300   Loss 3.37139869   Theta_1_0: 1.96749902 -6.98849058\n",
      "Epoch: 301   Loss 3.37138486   Theta_1_0: 1.96749961 -6.98811674\n",
      "Epoch: 302   Loss 3.37137079   Theta_1_0: 1.96750021 -6.98775005\n",
      "Epoch: 303   Loss 3.37135720   Theta_1_0: 1.96750069 -6.98739100\n",
      "Epoch: 304   Loss 3.37134480   Theta_1_0: 1.96750128 -6.98703909\n",
      "Epoch: 305   Loss 3.37133241   Theta_1_0: 1.96750176 -6.98669386\n",
      "Epoch: 306   Loss 3.37132025   Theta_1_0: 1.96750236 -6.98635578\n",
      "Epoch: 307   Loss 3.37130928   Theta_1_0: 1.96750283 -6.98602438\n",
      "Epoch: 308   Loss 3.37129831   Theta_1_0: 1.96750331 -6.98569965\n",
      "Epoch: 309   Loss 3.37128782   Theta_1_0: 1.96750379 -6.98538160\n",
      "Epoch: 310   Loss 3.37127805   Theta_1_0: 1.96750426 -6.98506975\n",
      "Epoch: 311   Loss 3.37126827   Theta_1_0: 1.96750474 -6.98476410\n",
      "Epoch: 312   Loss 3.37125874   Theta_1_0: 1.96750522 -6.98446465\n",
      "Epoch: 313   Loss 3.37124991   Theta_1_0: 1.96750569 -6.98417091\n",
      "Epoch: 314   Loss 3.37124157   Theta_1_0: 1.96750617 -6.98388338\n",
      "Epoch: 315   Loss 3.37123346   Theta_1_0: 1.96750665 -6.98360157\n",
      "Epoch: 316   Loss 3.37122560   Theta_1_0: 1.96750700 -6.98332548\n",
      "Epoch: 317   Loss 3.37121820   Theta_1_0: 1.96750748 -6.98305464\n",
      "Epoch: 318   Loss 3.37121105   Theta_1_0: 1.96750784 -6.98278952\n",
      "Epoch: 319   Loss 3.37120366   Theta_1_0: 1.96750820 -6.98252964\n",
      "Epoch: 320   Loss 3.37119699   Theta_1_0: 1.96750867 -6.98227501\n",
      "Epoch: 321   Loss 3.37119079   Theta_1_0: 1.96750903 -6.98202515\n",
      "Epoch: 322   Loss 3.37118459   Theta_1_0: 1.96750939 -6.98178053\n",
      "Epoch: 323   Loss 3.37117863   Theta_1_0: 1.96750975 -6.98154068\n",
      "Epoch: 324   Loss 3.37117314   Theta_1_0: 1.96751010 -6.98130560\n",
      "Epoch: 325   Loss 3.37116718   Theta_1_0: 1.96751046 -6.98107529\n",
      "Epoch: 326   Loss 3.37116241   Theta_1_0: 1.96751082 -6.98084974\n",
      "Epoch: 327   Loss 3.37115669   Theta_1_0: 1.96751118 -6.98062849\n",
      "Epoch: 328   Loss 3.37115192   Theta_1_0: 1.96751153 -6.98041153\n",
      "Epoch: 329   Loss 3.37114787   Theta_1_0: 1.96751189 -6.98019886\n",
      "Epoch: 330   Loss 3.37114286   Theta_1_0: 1.96751213 -6.97999048\n",
      "Epoch: 331   Loss 3.37113881   Theta_1_0: 1.96751249 -6.97978640\n",
      "Epoch: 332   Loss 3.37113476   Theta_1_0: 1.96751285 -6.97958660\n",
      "Epoch: 333   Loss 3.37113094   Theta_1_0: 1.96751308 -6.97939062\n",
      "Epoch: 334   Loss 3.37112665   Theta_1_0: 1.96751332 -6.97919846\n",
      "Epoch: 335   Loss 3.37112308   Theta_1_0: 1.96751368 -6.97901011\n",
      "Epoch: 336   Loss 3.37111998   Theta_1_0: 1.96751392 -6.97882557\n",
      "Epoch: 337   Loss 3.37111640   Theta_1_0: 1.96751428 -6.97864485\n",
      "Epoch: 338   Loss 3.37111330   Theta_1_0: 1.96751451 -6.97846794\n",
      "Epoch: 339   Loss 3.37110996   Theta_1_0: 1.96751475 -6.97829437\n",
      "Epoch: 340   Loss 3.37110686   Theta_1_0: 1.96751499 -6.97812414\n",
      "Epoch: 341   Loss 3.37110424   Theta_1_0: 1.96751523 -6.97795725\n",
      "Epoch: 342   Loss 3.37110162   Theta_1_0: 1.96751559 -6.97779369\n",
      "Epoch: 343   Loss 3.37109852   Theta_1_0: 1.96751583 -6.97763348\n",
      "Epoch: 344   Loss 3.37109613   Theta_1_0: 1.96751606 -6.97747660\n",
      "Epoch: 345   Loss 3.37109375   Theta_1_0: 1.96751630 -6.97732306\n",
      "Epoch: 346   Loss 3.37109137   Theta_1_0: 1.96751654 -6.97717237\n",
      "Epoch: 347   Loss 3.37108946   Theta_1_0: 1.96751678 -6.97702456\n",
      "Epoch: 348   Loss 3.37108707   Theta_1_0: 1.96751702 -6.97688007\n",
      "Epoch: 349   Loss 3.37108493   Theta_1_0: 1.96751726 -6.97673845\n",
      "Epoch: 350   Loss 3.37108302   Theta_1_0: 1.96751750 -6.97659969\n",
      "Epoch: 351   Loss 3.37108088   Theta_1_0: 1.96751761 -6.97646332\n",
      "Epoch: 352   Loss 3.37107921   Theta_1_0: 1.96751785 -6.97632980\n",
      "Epoch: 353   Loss 3.37107730   Theta_1_0: 1.96751809 -6.97619915\n",
      "Epoch: 354   Loss 3.37107563   Theta_1_0: 1.96751821 -6.97607088\n",
      "Epoch: 355   Loss 3.37107420   Theta_1_0: 1.96751845 -6.97594547\n",
      "Epoch: 356   Loss 3.37107229   Theta_1_0: 1.96751857 -6.97582245\n",
      "Epoch: 357   Loss 3.37107110   Theta_1_0: 1.96751881 -6.97570181\n",
      "Epoch: 358   Loss 3.37106991   Theta_1_0: 1.96751893 -6.97558355\n",
      "Epoch: 359   Loss 3.37106848   Theta_1_0: 1.96751916 -6.97546768\n",
      "Epoch: 360   Loss 3.37106681   Theta_1_0: 1.96751928 -6.97535419\n",
      "Epoch: 361   Loss 3.37106562   Theta_1_0: 1.96751952 -6.97524309\n",
      "Epoch: 362   Loss 3.37106442   Theta_1_0: 1.96751964 -6.97513390\n",
      "Epoch: 363   Loss 3.37106323   Theta_1_0: 1.96751988 -6.97502708\n",
      "Epoch: 364   Loss 3.37106228   Theta_1_0: 1.96752000 -6.97492218\n",
      "Epoch: 365   Loss 3.37106085   Theta_1_0: 1.96752012 -6.97481966\n",
      "Epoch: 366   Loss 3.37106013   Theta_1_0: 1.96752036 -6.97471905\n",
      "Epoch: 367   Loss 3.37105894   Theta_1_0: 1.96752048 -6.97462034\n",
      "Epoch: 368   Loss 3.37105799   Theta_1_0: 1.96752059 -6.97452354\n",
      "Epoch: 369   Loss 3.37105703   Theta_1_0: 1.96752071 -6.97442865\n",
      "Epoch: 370   Loss 3.37105656   Theta_1_0: 1.96752083 -6.97433567\n",
      "Epoch: 371   Loss 3.37105536   Theta_1_0: 1.96752107 -6.97424459\n",
      "Epoch: 372   Loss 3.37105441   Theta_1_0: 1.96752119 -6.97415543\n",
      "Epoch: 373   Loss 3.37105370   Theta_1_0: 1.96752131 -6.97406816\n",
      "Epoch: 374   Loss 3.37105322   Theta_1_0: 1.96752143 -6.97398281\n",
      "Epoch: 375   Loss 3.37105227   Theta_1_0: 1.96752155 -6.97389889\n",
      "Epoch: 376   Loss 3.37105131   Theta_1_0: 1.96752167 -6.97381687\n",
      "Epoch: 377   Loss 3.37105107   Theta_1_0: 1.96752179 -6.97373629\n",
      "Epoch: 378   Loss 3.37105036   Theta_1_0: 1.96752191 -6.97365713\n",
      "Epoch: 379   Loss 3.37104988   Theta_1_0: 1.96752203 -6.97357988\n",
      "Epoch: 380   Loss 3.37104917   Theta_1_0: 1.96752214 -6.97350407\n",
      "Epoch: 381   Loss 3.37104869   Theta_1_0: 1.96752226 -6.97342968\n",
      "Epoch: 382   Loss 3.37104774   Theta_1_0: 1.96752238 -6.97335672\n",
      "Epoch: 383   Loss 3.37104726   Theta_1_0: 1.96752250 -6.97328520\n",
      "Epoch: 384   Loss 3.37104678   Theta_1_0: 1.96752262 -6.97321510\n",
      "Epoch: 385   Loss 3.37104678   Theta_1_0: 1.96752274 -6.97314644\n",
      "Epoch: 386   Loss 3.37104607   Theta_1_0: 1.96752286 -6.97307920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 387   Loss 3.37104559   Theta_1_0: 1.96752298 -6.97301340\n",
      "Epoch: 388   Loss 3.37104487   Theta_1_0: 1.96752310 -6.97294903\n",
      "Epoch: 389   Loss 3.37104464   Theta_1_0: 1.96752310 -6.97288561\n",
      "Epoch: 390   Loss 3.37104440   Theta_1_0: 1.96752322 -6.97282362\n",
      "Epoch: 391   Loss 3.37104368   Theta_1_0: 1.96752334 -6.97276306\n",
      "Epoch: 392   Loss 3.37104368   Theta_1_0: 1.96752346 -6.97270346\n",
      "Epoch: 393   Loss 3.37104273   Theta_1_0: 1.96752357 -6.97264528\n",
      "Epoch: 394   Loss 3.37104273   Theta_1_0: 1.96752357 -6.97258806\n",
      "Epoch: 395   Loss 3.37104249   Theta_1_0: 1.96752369 -6.97253227\n",
      "Epoch: 396   Loss 3.37104249   Theta_1_0: 1.96752381 -6.97247744\n",
      "Epoch: 397   Loss 3.37104225   Theta_1_0: 1.96752393 -6.97242355\n",
      "Epoch: 398   Loss 3.37104177   Theta_1_0: 1.96752393 -6.97237062\n",
      "Epoch: 399   Loss 3.37104130   Theta_1_0: 1.96752405 -6.97231913\n",
      "Epoch: 400   Loss 3.37104130   Theta_1_0: 1.96752405 -6.97226858\n",
      "Epoch: 401   Loss 3.37104058   Theta_1_0: 1.96752417 -6.97221899\n",
      "Epoch: 402   Loss 3.37104058   Theta_1_0: 1.96752429 -6.97217035\n",
      "Epoch: 403   Loss 3.37104011   Theta_1_0: 1.96752429 -6.97212267\n",
      "Epoch: 404   Loss 3.37104034   Theta_1_0: 1.96752441 -6.97207594\n",
      "Epoch: 405   Loss 3.37104011   Theta_1_0: 1.96752441 -6.97203016\n",
      "Epoch: 406   Loss 3.37103939   Theta_1_0: 1.96752453 -6.97198534\n",
      "Epoch: 407   Loss 3.37103963   Theta_1_0: 1.96752465 -6.97194147\n",
      "Epoch: 408   Loss 3.37103963   Theta_1_0: 1.96752465 -6.97189856\n",
      "Epoch: 409   Loss 3.37103939   Theta_1_0: 1.96752477 -6.97185612\n",
      "Epoch: 410   Loss 3.37103891   Theta_1_0: 1.96752477 -6.97181463\n",
      "Epoch: 411   Loss 3.37103891   Theta_1_0: 1.96752489 -6.97177410\n",
      "Epoch: 412   Loss 3.37103844   Theta_1_0: 1.96752489 -6.97173452\n",
      "Epoch: 413   Loss 3.37103820   Theta_1_0: 1.96752501 -6.97169542\n",
      "Epoch: 414   Loss 3.37103820   Theta_1_0: 1.96752501 -6.97165728\n",
      "Epoch: 415   Loss 3.37103820   Theta_1_0: 1.96752512 -6.97162008\n",
      "Epoch: 416   Loss 3.37103820   Theta_1_0: 1.96752512 -6.97158337\n",
      "Epoch: 417   Loss 3.37103844   Theta_1_0: 1.96752524 -6.97154760\n",
      "Epoch: 418   Loss 3.37103748   Theta_1_0: 1.96752524 -6.97151232\n",
      "Epoch: 419   Loss 3.37103748   Theta_1_0: 1.96752536 -6.97147799\n",
      "Epoch: 420   Loss 3.37103796   Theta_1_0: 1.96752536 -6.97144413\n",
      "Epoch: 421   Loss 3.37103748   Theta_1_0: 1.96752548 -6.97141123\n",
      "Epoch: 422   Loss 3.37103724   Theta_1_0: 1.96752548 -6.97137880\n",
      "Epoch: 423   Loss 3.37103748   Theta_1_0: 1.96752560 -6.97134686\n",
      "Epoch: 424   Loss 3.37103724   Theta_1_0: 1.96752560 -6.97131586\n",
      "Epoch: 425   Loss 3.37103701   Theta_1_0: 1.96752560 -6.97128534\n",
      "Epoch: 426   Loss 3.37103701   Theta_1_0: 1.96752560 -6.97125530\n",
      "Epoch: 427   Loss 3.37103724   Theta_1_0: 1.96752572 -6.97122574\n",
      "Epoch: 428   Loss 3.37103677   Theta_1_0: 1.96752572 -6.97119713\n",
      "Epoch: 429   Loss 3.37103677   Theta_1_0: 1.96752584 -6.97116899\n",
      "Epoch: 430   Loss 3.37103701   Theta_1_0: 1.96752584 -6.97114134\n",
      "Epoch: 431   Loss 3.37103677   Theta_1_0: 1.96752584 -6.97111416\n",
      "Epoch: 432   Loss 3.37103629   Theta_1_0: 1.96752596 -6.97108746\n",
      "Epoch: 433   Loss 3.37103629   Theta_1_0: 1.96752596 -6.97106171\n",
      "Epoch: 434   Loss 3.37103629   Theta_1_0: 1.96752596 -6.97103643\n",
      "Epoch: 435   Loss 3.37103581   Theta_1_0: 1.96752608 -6.97101164\n",
      "Epoch: 436   Loss 3.37103629   Theta_1_0: 1.96752608 -6.97098732\n",
      "Epoch: 437   Loss 3.37103605   Theta_1_0: 1.96752608 -6.97096348\n",
      "Epoch: 438   Loss 3.37103605   Theta_1_0: 1.96752620 -6.97094011\n",
      "Epoch: 439   Loss 3.37103605   Theta_1_0: 1.96752620 -6.97091722\n",
      "Epoch: 440   Loss 3.37103629   Theta_1_0: 1.96752620 -6.97089481\n",
      "Epoch: 441   Loss 3.37103581   Theta_1_0: 1.96752620 -6.97087288\n",
      "Epoch: 442   Loss 3.37103581   Theta_1_0: 1.96752632 -6.97085142\n",
      "Epoch: 443   Loss 3.37103605   Theta_1_0: 1.96752632 -6.97082996\n",
      "Epoch: 444   Loss 3.37103581   Theta_1_0: 1.96752632 -6.97080898\n",
      "Epoch: 445   Loss 3.37103581   Theta_1_0: 1.96752644 -6.97078848\n",
      "Epoch: 446   Loss 3.37103558   Theta_1_0: 1.96752644 -6.97076845\n",
      "Epoch: 447   Loss 3.37103558   Theta_1_0: 1.96752644 -6.97074890\n",
      "Epoch: 448   Loss 3.37103581   Theta_1_0: 1.96752644 -6.97072983\n",
      "Epoch: 449   Loss 3.37103558   Theta_1_0: 1.96752644 -6.97071123\n",
      "Epoch: 450   Loss 3.37103558   Theta_1_0: 1.96752656 -6.97069263\n",
      "Epoch: 451   Loss 3.37103558   Theta_1_0: 1.96752656 -6.97067451\n",
      "Epoch: 452   Loss 3.37103558   Theta_1_0: 1.96752656 -6.97065687\n",
      "Epoch: 453   Loss 3.37103581   Theta_1_0: 1.96752656 -6.97063971\n",
      "Epoch: 454   Loss 3.37103558   Theta_1_0: 1.96752667 -6.97062254\n",
      "Epoch: 455   Loss 3.37103558   Theta_1_0: 1.96752667 -6.97060585\n",
      "Epoch: 456   Loss 3.37103510   Theta_1_0: 1.96752667 -6.97058964\n",
      "Epoch: 457   Loss 3.37103510   Theta_1_0: 1.96752667 -6.97057343\n",
      "Epoch: 458   Loss 3.37103558   Theta_1_0: 1.96752679 -6.97055769\n",
      "Epoch: 459   Loss 3.37103558   Theta_1_0: 1.96752679 -6.97054243\n",
      "Epoch: 460   Loss 3.37103510   Theta_1_0: 1.96752679 -6.97052717\n",
      "Epoch: 461   Loss 3.37103510   Theta_1_0: 1.96752679 -6.97051239\n",
      "Epoch: 462   Loss 3.37103510   Theta_1_0: 1.96752679 -6.97049809\n",
      "Epoch: 463   Loss 3.37103510   Theta_1_0: 1.96752679 -6.97048378\n",
      "Epoch: 464   Loss 3.37103510   Theta_1_0: 1.96752691 -6.97046995\n",
      "Epoch: 465   Loss 3.37103486   Theta_1_0: 1.96752691 -6.97045612\n",
      "Epoch: 466   Loss 3.37103510   Theta_1_0: 1.96752691 -6.97044277\n",
      "Epoch: 467   Loss 3.37103510   Theta_1_0: 1.96752691 -6.97042990\n",
      "Epoch: 468   Loss 3.37103486   Theta_1_0: 1.96752691 -6.97041702\n",
      "Epoch: 469   Loss 3.37103510   Theta_1_0: 1.96752691 -6.97040462\n",
      "Epoch: 470   Loss 3.37103510   Theta_1_0: 1.96752703 -6.97039223\n",
      "Epoch: 471   Loss 3.37103486   Theta_1_0: 1.96752703 -6.97038031\n",
      "Epoch: 472   Loss 3.37103486   Theta_1_0: 1.96752703 -6.97036839\n",
      "Epoch: 473   Loss 3.37103510   Theta_1_0: 1.96752703 -6.97035694\n",
      "Epoch: 474   Loss 3.37103510   Theta_1_0: 1.96752703 -6.97034550\n",
      "Epoch: 475   Loss 3.37103486   Theta_1_0: 1.96752703 -6.97033453\n",
      "Epoch: 476   Loss 3.37103510   Theta_1_0: 1.96752715 -6.97032356\n",
      "Epoch: 477   Loss 3.37103510   Theta_1_0: 1.96752715 -6.97031307\n",
      "Epoch: 478   Loss 3.37103510   Theta_1_0: 1.96752715 -6.97030258\n",
      "Epoch: 479   Loss 3.37103510   Theta_1_0: 1.96752715 -6.97029209\n",
      "Epoch: 480   Loss 3.37103486   Theta_1_0: 1.96752715 -6.97028208\n",
      "Epoch: 481   Loss 3.37103486   Theta_1_0: 1.96752715 -6.97027206\n"
     ]
    }
   ],
   "source": [
    "# Use the below line in Jupyter Notebook to view the plot with interactive window, it doesn't work in Google Colab\n",
    "%matplotlib qt\n",
    "plt.xlim(-15, 15)\n",
    "plt.ylim(-30, 20)\n",
    "\n",
    "plt.plot(x, Y, \"ro\")\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "epoch = 1000\n",
    "count_epoch = 1\n",
    "while True:\n",
    "    # Calculate loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = Loss(theta)\n",
    "\n",
    "    grads = tape.gradient(loss, [theta])\n",
    "    \n",
    "    \n",
    "    # Draw a line using theta to see how the algorithm is doing\n",
    "    x_vis = np.array([-15.0, 15.0])\n",
    "    y_vis = theta[1][0]*x_vis + theta[0][0]\n",
    "    plt.plot(x_vis, y_vis)\n",
    "    plt.pause(0.01)\n",
    "\n",
    "    # Update theta\n",
    "    opt.apply_gradients(zip(grads, [theta]))\n",
    "    \n",
    "    print(\"Epoch: %d   Loss %.8f   Theta_1_0: %.8f %.8f\" % (count_epoch, loss.numpy(), theta[1][0], theta[0][0]))\n",
    "    \n",
    "    # Check if all values in loss is less than epsilon\n",
    "    # It will probably take a long time to satisfy the above condition, so we will have to use epoch\n",
    "    if tf.experimental.numpy.all((tf.math.abs(grads) < epsilon)) or count_epoch == epoch:\n",
    "        break   \n",
    "    count_epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaIyBaRsea4W"
   },
   "source": [
    "**Result:** theta after running Linear Regression<br>\n",
    "**Note:** The algorithm runs **correctly** when **theta_1** and **theta_0** values **approximate** the **weight** and **bias** values of the line equation in the training data generation part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PXcmsRzCei7d",
    "outputId": "d232307e-2fd6-46c5-d797-6759cf07b834"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gi tr ti u hin ti ca theta_1:  1.9675272\n",
      "Gi tr ti u hin ti ca theta_0:  -6.970272\n"
     ]
    }
   ],
   "source": [
    "print(\"Gi tr ti u hin ti ca theta_1: \", theta[1][0].numpy())\n",
    "print(\"Gi tr ti u hin ti ca theta_0: \", theta[0][0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdeUCU9LgvGc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LinearRegression_Tensorflow[Upgrade].ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
